{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #8 (Due 11/06/2019, 11:59pm)\n",
    "## Variational Inference for Bayesian Neural Networks\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Cooper Lorsung\n",
    "\n",
    "**Students collaborators:** Sujay Thakur, Jovin Leong, Brian Chu, Benjamin Levy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam, sgd\n",
    "from autograd import scipy as sp\n",
    "#from autograd.scipy.stats import bernoulli\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "#from nn_models import Feedforward\n",
    "#from bayesian_regression import Bayesian_Regression\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description: Bayesian Neural Network Regression\n",
    "In Homework #7, you explored sampling from the posteriors of ***Bayesian neural networks*** using HMC. In Lab #8 you'll explore the extent to which HMC can be inefficient or ineffective for sampling from certain types of posteriors. In this homework, you will study variational approximations of BNN posteriors, especially when compared to the posteriors obtained by sampling (in Homework #7). The data is the same as the one for Homework #7.\n",
    "\n",
    "\n",
    "### Part I: Implement Black-Box Variational Inference with the Reparametrization Trick\n",
    "\n",
    "1. (**BBVI with the Reparametrization Trick**) Implement BBVI with the reparametrization trick for approximating an arbitrary posterior $p(w| \\text{Data})$ by an isotropic Gaussian $\\mathcal{N}(\\mu, \\Sigma)$, where $\\Sigma$ is a diagonal matrix. See Lecture #15 or the example code from [autograd's github repo](https://github.com/HIPS/autograd/blob/master/examples/black_box_svi.py). \n",
    "<br><br>\n",
    "\n",
    "2. (**Unit Test**) Check that your implementation is correct by approximating the posterior of the following Bayesian logistic regression model:\n",
    "\\begin{align}\n",
    "w &\\sim \\mathcal{N}(0, 1)\\\\\n",
    "Y^{(n)} &\\sim Ber(\\text{sigm}(wX^{(n)} + 10))\n",
    "\\end{align}\n",
    "  where $w$, $Y^{(n)}$, $X^{(n)}$ are a real scalar valued random variables, and where the data consists of a single observation $(Y=1, X=-20)$.\n",
    "\n",
    "  The true posterior $p(w | Y=1, X=-20)$ should look like the following (i.e. the true posterior is left-skewed):\n",
    "<img src=\"./logistic_posterior.png\" style='height:200px;'>\n",
    "  Your mean-field variational approximation should be a Gaussian with mean -0.321 and standard deviation 0.876 (all approximate).\n",
    "<br><br>\n",
    "\n",
    "### Part II: Approximate the Posterior of a Bayesian Neural Network\n",
    "\n",
    "1. (**Variational Inference for BNNs**) We will implement the following Bayesian model for the data:\n",
    "\\begin{align}\n",
    "\\mathbf{W} &\\sim \\mathcal{N}(0, 5^2 \\mathbf{I}_{D\\times D})\\\\\n",
    "\\mu^{(n)} &= g_{\\mathbf{W}}(\\mathbf{X}^{(n)})\\\\\n",
    "Y^{(n)} &\\sim \\mathcal{N}(\\mu^{(n)}, 0.5^2)\n",
    "\\end{align}\n",
    "  where $g_{\\mathbf{W}}$ is a neural network with parameters $\\mathbf{W}$ represented as a vector in $\\mathbb{R}^{D}$ with $D$ being the total number of parameters (including biases). Just as in HW #7, use a network with a single hidden layer, 5 hidden nodes and rbf activation function.\n",
    "\n",
    "  Implement the log of the joint distribution in `autograd`'s version of `numpy`, i.e. implement $\\log \\left[p(\\mathbf{W})\\prod_{n=1}^N p(Y^{(n)} |\\mathbf{X}^{(n)} , \\mathbf{W}) \\right]$.\n",
    "  \n",
    "  ***Hint:*** you'll need to write out the log of the various Gaussian pdf's and implement their formulae using `autograd`'s numpy functions.<br><br>\n",
    "\n",
    "4. (**Approximate the Posterior**) Use BBVI with the reparametrization trick to approximate the posterior of the Bayesian neural network with a mean-field Gaussian variational family (i.e. an isotropic Gaussian). Please set learning rate and maximum iteration choices as you see fit!<br><br>\n",
    "  \n",
    "4. (**Visualize the Posterior Predictive**) Visualize 100 samples $\\mathbf{W}^s$ from your approximate posterior of $\\mathbf{W}$ by ploting the neural network outputs with weight $\\mathbf{W}^s$ plus a random noise $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$ at 100 equally spaced x-values between -8 and 8:\n",
    "``` python \n",
    "x_test = np.linspace(-8, 8, 100)\n",
    "y_test = nn.forward(sample, x_test.reshape((1, -1)))\n",
    "```\n",
    "  where `sample` is a sample from the approximate posterior of $\\mathbf{W}$.<br><br>\n",
    "\n",
    "5. (**Computing the Fit**) Compute the posterior predictive log likelihood of the observed data under your model. \n",
    "<br><br>\n",
    "  \n",
    "6. (**Model Evaluation**) Compare the posterior predictive visualization and the posterior predictive log likelihood obtained from BBVI with the reparametrization trick to the ones you obtained in HW #7. Can you say whether or not your posterior approximation is good? How does approximating the posterior effect our estimation of epistemic and aleatoric uncertainty?<br><br>\n",
    "\n",
    "7. (**Extra Credit**) Get your HMC sampler to converge for this BNN model and this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward:\n",
    "    def __init__(self, architecture, random=None, weights=None):\n",
    "        self.params = {'H': architecture['width'],\n",
    "                       'L': architecture['hidden_layers'],\n",
    "                       'D_in': architecture['input_dim'],\n",
    "                       'D_out': architecture['output_dim'],\n",
    "                       'activation_type': architecture['activation_fn_type'],\n",
    "                       'activation_params': architecture['activation_fn_params']}\n",
    "\n",
    "        self.D = (  (architecture['input_dim'] * architecture['width'] + architecture['width'])\n",
    "                  + (architecture['output_dim'] * architecture['width'] + architecture['output_dim'])\n",
    "                  + (architecture['hidden_layers'] - 1) * (architecture['width']**2 + architecture['width'])\n",
    "                 )\n",
    "\n",
    "        if random is not None:\n",
    "            self.random = random\n",
    "        else:\n",
    "            self.random = np.random.RandomState(0)\n",
    "\n",
    "        self.h = architecture['activation_fn']\n",
    "\n",
    "        if weights is None:\n",
    "            self.weights = self.random.normal(0, 1, size=(1, self.D))\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        self.objective_trace = np.empty((1, 1))\n",
    "        self.weight_trace = np.empty((1, self.D))\n",
    "\n",
    "\n",
    "    def forward(self, weights, x):\n",
    "        ''' Forward pass given weights and input '''\n",
    "        H = self.params['H']\n",
    "        D_in = self.params['D_in']\n",
    "        D_out = self.params['D_out']\n",
    "\n",
    "        assert weights.shape[1] == self.D\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            assert x.shape[0] == D_in\n",
    "            x = x.reshape((1, D_in, -1))\n",
    "        else:\n",
    "            assert x.shape[1] == D_in\n",
    "\n",
    "        weights = weights.T\n",
    "\n",
    "\n",
    "        #input to first hidden layer\n",
    "        W = weights[:H * D_in].T.reshape((-1, H, D_in))\n",
    "        b = weights[H * D_in:H * D_in + H].T.reshape((-1, H, 1))\n",
    "        input = self.h(np.matmul(W, x) + b)\n",
    "        index = H * D_in + H\n",
    "\n",
    "        assert input.shape[1] == H\n",
    "\n",
    "        #additional hidden layers\n",
    "        for _ in range(self.params['L'] - 1):\n",
    "            before = index\n",
    "            W = weights[index:index + H * H].T.reshape((-1, H, H))\n",
    "            index += H * H\n",
    "            b = weights[index:index + H].T.reshape((-1, H, 1))\n",
    "            index += H\n",
    "            output = np.matmul(W, input) + b\n",
    "            input = self.h(output)\n",
    "\n",
    "            assert input.shape[1] == H\n",
    "\n",
    "        #output layer\n",
    "        W = weights[index:index + H * D_out].T.reshape((-1, D_out, H))\n",
    "        b = weights[index + H * D_out:].T.reshape((-1, D_out, 1))\n",
    "        output = np.matmul(W, input) + b\n",
    "        assert output.shape[1] == self.params['D_out']\n",
    "\n",
    "        return output\n",
    "\n",
    "    def make_objective(self, x_train, y_train, reg_param=None):\n",
    "        ''' Make objective functions: depending on whether or not you want to apply l2 regularization '''\n",
    "        \n",
    "        if reg_param is None:\n",
    "            \n",
    "            def objective(W, t):\n",
    "                squared_error = np.linalg.norm(y_train - self.forward(W, x_train), axis=1)**2\n",
    "                sum_error = np.sum(squared_error)\n",
    "                return sum_error\n",
    "            \n",
    "            return objective, grad(objective)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            def objective(W, t):\n",
    "                squared_error = np.linalg.norm(y_train - self.forward(W, x_train), axis=1)**2\n",
    "                mean_error = np.mean(squared_error) + reg_param * np.linalg.norm(W)\n",
    "                return mean_error\n",
    "            \n",
    "            return objective, grad(objective)\n",
    "\n",
    "    def fit(self, x_train, y_train, params, reg_param=None):\n",
    "        ''' Wrapper for MLE through gradient descent '''\n",
    "        assert x_train.shape[0] == self.params['D_in']\n",
    "        assert y_train.shape[0] == self.params['D_out']\n",
    "\n",
    "        ### make objective function for training\n",
    "        self.objective, self.gradient = self.make_objective(x_train, y_train, reg_param)\n",
    "\n",
    "        ### set up optimization\n",
    "        step_size = 0.01\n",
    "        max_iteration = 5000\n",
    "        check_point = 100\n",
    "        weights_init = self.weights.reshape((1, -1))\n",
    "        mass = None\n",
    "        optimizer = 'adam'\n",
    "        random_restarts = 5\n",
    "\n",
    "        if 'step_size' in params.keys():\n",
    "            step_size = params['step_size']\n",
    "        if 'max_iteration' in params.keys():\n",
    "            max_iteration = params['max_iteration']\n",
    "        if 'check_point' in params.keys():\n",
    "            self.check_point = params['check_point']\n",
    "        if 'init' in params.keys():\n",
    "            weights_init = params['init']\n",
    "        if 'call_back' in params.keys():\n",
    "            call_back = params['call_back']\n",
    "        if 'mass' in params.keys():\n",
    "            mass = params['mass']\n",
    "        if 'optimizer' in params.keys():\n",
    "            optimizer = params['optimizer']\n",
    "        if 'random_restarts' in params.keys():\n",
    "            random_restarts = params['random_restarts']\n",
    "\n",
    "        def call_back(weights, iteration, g):\n",
    "            ''' Actions per optimization step '''\n",
    "            objective = self.objective(weights, iteration)\n",
    "            self.objective_trace = np.vstack((self.objective_trace, objective))\n",
    "            self.weight_trace = np.vstack((self.weight_trace, weights))\n",
    "            if iteration % check_point == 0:\n",
    "                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(iteration, objective, np.linalg.norm(self.gradient(weights, iteration))))\n",
    "\n",
    "        ### train with random restarts\n",
    "        optimal_obj = 1e16\n",
    "        optimal_weights = self.weights\n",
    "\n",
    "        for i in range(random_restarts):\n",
    "            if optimizer == 'adam':\n",
    "                adam(self.gradient, weights_init, step_size=step_size, num_iters=max_iteration, callback=call_back)\n",
    "            local_opt = np.min(self.objective_trace[-100:])\n",
    "            \n",
    "            if local_opt < optimal_obj:\n",
    "                opt_index = np.argmin(self.objective_trace[-100:])\n",
    "                self.weights = self.weight_trace[-100:][opt_index].reshape((1, -1))\n",
    "            weights_init = self.random.normal(0, 1, size=(1, self.D))\n",
    "\n",
    "        self.objective_trace = self.objective_trace[1:]\n",
    "        self.weight_trace = self.weight_trace[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_variational_inference(logprob, D, num_samples):\n",
    "    \"\"\"Implements http://arxiv.org/abs/1401.0118, and uses the\n",
    "    local reparameterization trick from http://arxiv.org/abs/1506.02557\"\"\"\n",
    "\n",
    "    def unpack_params(params):\n",
    "        # Variational dist is a diagonal Gaussian.\n",
    "        mean, log_std = params[:D], params[D:]\n",
    "        return mean, log_std\n",
    "\n",
    "    def gaussian_entropy(log_std):\n",
    "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
    "\n",
    "    rs = npr.RandomState(0)\n",
    "    def variational_objective(params, t):\n",
    "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
    "        mean, log_std = unpack_params(params)\n",
    "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
    "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
    "        return -lower_bound\n",
    "\n",
    "    gradient = grad(variational_objective)\n",
    "\n",
    "    return variational_objective, gradient, unpack_params\n",
    "    \n",
    "def variational_inference(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose):\n",
    "    '''implements wrapper for variational inference via bbb for bayesian regression'''\n",
    "    D = Sigma_W.shape[0]\n",
    "    Sigma_W_inv = np.linalg.inv(Sigma_W)\n",
    "    Sigma_W_det = np.linalg.det(Sigma_W)\n",
    "    variational_dim = D\n",
    "    \n",
    "    #define the log prior on the model parameters\n",
    "    def log_prior(W):\n",
    "        constant_W = -0.5 * (D * np.log(2 * np.pi) + np.log(Sigma_W_det))\n",
    "        exponential_W = -0.5 * np.diag(np.dot(np.dot(W, Sigma_W_inv), W.T))\n",
    "        log_p_W = constant_W + exponential_W\n",
    "        return log_p_W\n",
    "\n",
    "    #define the log likelihood\n",
    "    def log_lklhd(W):\n",
    "        S = W.shape[0]\n",
    "        constant = (-np.log(sigma_y) - 0.5 * np.log(2 * np.pi)) * N\n",
    "        exponential = -0.5 * sigma_y**-2 * np.sum((y_train.reshape((1, 1, N)) - forward(W, x_train))**2, axis=2).flatten()\n",
    "        return constant + exponential\n",
    "    log_density = lambda w, t: log_lklhd(w) + log_prior(w)\n",
    "\n",
    "    #build variational objective.\n",
    "    objective, gradient, unpack_params = black_box_variational_inference(log_density, D, num_samples=S)\n",
    "\n",
    "    def callback(params, t, g):\n",
    "        if(verbose and t!=0):\n",
    "            if  t % 100 == 0:\n",
    "                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(t, -objective(params, t), np.linalg.norm(gradient(params, t))))\n",
    "\n",
    "    print(\"Optimizing variational parameters...\")\n",
    "    #initialize variational parameters\n",
    "    init_mean = np.ones(D)\n",
    "    init_log_std = -100 * np.ones(D)\n",
    "    init_var_params = np.concatenate([init_mean, init_log_std])\n",
    "    \n",
    "    #perform gradient descent using adam (a type of gradient-based optimizer)\n",
    "    variational_params = adam(gradient, init_var_params, step_size=step_size, num_iters=max_iteration, callback=callback)\n",
    "    \n",
    "    return variational_params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "Questions 1 and 2 are implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 lower bound -2.581312405632847; gradient mag: 4.142926414068996\n",
      "MEAN: -0.3, VARIANCE: -0.11\n",
      "\n",
      "Iteration 100 lower bound -2.081837995564131; gradient mag: 6.819361978860413\n",
      "MEAN: -0.309738692262109, VARIANCE: -0.11980160568918605\n",
      "\n",
      "Iteration 200 lower bound -2.3256254451240608; gradient mag: 8.111130813887884\n",
      "MEAN: -0.31957524220055705, VARIANCE: -0.12960963980035817\n",
      "\n",
      "Iteration 300 lower bound -2.6614157927537745; gradient mag: 5.983143593365274\n",
      "MEAN: -0.3290215339891909, VARIANCE: -0.13903617867990595\n",
      "\n",
      "Iteration 400 lower bound -1.690362484871129; gradient mag: 2.775479543219632\n",
      "MEAN: -0.33831684700509745, VARIANCE: -0.14838363411075617\n",
      "\n",
      "Iteration 500 lower bound -1.071351384696401; gradient mag: 5.487612635617772\n",
      "MEAN: -0.3473606890379668, VARIANCE: -0.15742783800078275\n",
      "\n",
      "Iteration 600 lower bound -1.5709853274339327; gradient mag: 4.2566410006707045\n",
      "MEAN: -0.3565335026838002, VARIANCE: -0.16662557617454118\n",
      "\n",
      "Iteration 700 lower bound -1.5621107893893673; gradient mag: 6.085567921764449\n",
      "MEAN: -0.3651624401666619, VARIANCE: -0.17531894793785108\n",
      "\n",
      "Iteration 800 lower bound -1.6302707396819716; gradient mag: 6.948244617905518\n",
      "MEAN: -0.3738914804517776, VARIANCE: -0.18418260583670829\n",
      "\n",
      "Iteration 900 lower bound -0.9889833817248457; gradient mag: 3.281100713447275\n",
      "MEAN: -0.3825291058615301, VARIANCE: -0.19293687949905042\n",
      "\n",
      "Iteration 1000 lower bound -1.3312109275726116; gradient mag: 4.708667409232525\n",
      "MEAN: -0.39115629810914465, VARIANCE: -0.20166904716450545\n",
      "\n",
      "Iteration 1100 lower bound -0.9143951263148611; gradient mag: 2.5635455856774745\n",
      "MEAN: -0.3994299068513016, VARIANCE: -0.21006853510422382\n",
      "\n",
      "Iteration 1200 lower bound -1.5744696734206334; gradient mag: 2.4693670238689336\n",
      "MEAN: -0.4074927216435385, VARIANCE: -0.21828552841680585\n",
      "\n",
      "Iteration 1300 lower bound -1.1297308372855233; gradient mag: 1.9919338096922499\n",
      "MEAN: -0.4156693978075829, VARIANCE: -0.22654892739881466\n",
      "\n",
      "Iteration 1400 lower bound -0.9701041386411802; gradient mag: 4.236123642590417\n",
      "MEAN: -0.4234134739839611, VARIANCE: -0.23449316932691816\n",
      "\n",
      "Iteration 1500 lower bound -1.1635187320299323; gradient mag: 2.0349808132073766\n",
      "MEAN: -0.43102930223196356, VARIANCE: -0.2422291445120724\n",
      "\n",
      "Iteration 1600 lower bound -1.4865919056902899; gradient mag: 3.0556513630755058\n",
      "MEAN: -0.4384819143009258, VARIANCE: -0.2498710533658281\n",
      "\n",
      "Iteration 1700 lower bound -0.7362882333623613; gradient mag: 1.8453422738486633\n",
      "MEAN: -0.4458753740453151, VARIANCE: -0.2574213356619218\n",
      "\n",
      "Iteration 1800 lower bound -0.881171374524051; gradient mag: 5.759284361863723\n",
      "MEAN: -0.4528644455937472, VARIANCE: -0.2646171962353393\n",
      "\n",
      "Iteration 1900 lower bound -0.7969067143156392; gradient mag: 1.7729962885623674\n",
      "MEAN: -0.45986298083349914, VARIANCE: -0.2718952100996528\n",
      "\n",
      "Iteration 2000 lower bound -0.9744151313313625; gradient mag: 2.8050103398534025\n",
      "MEAN: -0.4667297912453511, VARIANCE: -0.2790568952674411\n",
      "\n",
      "Iteration 2100 lower bound -0.6468697996614341; gradient mag: 3.539968564238762\n",
      "MEAN: -0.47357223533580617, VARIANCE: -0.2862265072061601\n",
      "\n",
      "Iteration 2200 lower bound -0.8442883362544629; gradient mag: 1.1836158026922752\n",
      "MEAN: -0.48034522561291004, VARIANCE: -0.2932625978908546\n",
      "\n",
      "Iteration 2300 lower bound -0.7969860856124831; gradient mag: 0.7092628452676104\n",
      "MEAN: -0.4866919316977555, VARIANCE: -0.29992442368837297\n",
      "\n",
      "Iteration 2400 lower bound -0.7651569760105972; gradient mag: 1.7956912470205195\n",
      "MEAN: -0.4928779785498722, VARIANCE: -0.30650849154073984\n",
      "\n",
      "Iteration 2500 lower bound -0.41207261540304274; gradient mag: 3.47082334205921\n",
      "MEAN: -0.49946682363449535, VARIANCE: -0.31340955305851725\n",
      "\n",
      "Iteration 2600 lower bound -0.5533935600678217; gradient mag: 0.8049404101613677\n",
      "MEAN: -0.5056394529931173, VARIANCE: -0.3200439365018484\n",
      "\n",
      "Iteration 2700 lower bound -1.7059377119763002; gradient mag: 0.7746802307251325\n",
      "MEAN: -0.5117705686794464, VARIANCE: -0.32651500479925755\n",
      "\n",
      "Iteration 2800 lower bound -0.8727390325279136; gradient mag: 2.733920057721655\n",
      "MEAN: -0.5179558243510048, VARIANCE: -0.3330361552085063\n",
      "\n",
      "Iteration 2900 lower bound -0.35693417616485346; gradient mag: 1.756161096628534\n",
      "MEAN: -0.524138313125191, VARIANCE: -0.3397475791094766\n",
      "\n",
      "Iteration 3000 lower bound -0.6578485522846378; gradient mag: 1.8229401290969351\n",
      "MEAN: -0.5299413754466734, VARIANCE: -0.346148559930043\n",
      "\n",
      "Iteration 3100 lower bound -0.6052790521622231; gradient mag: 0.6771399184620533\n",
      "MEAN: -0.535587044208681, VARIANCE: -0.352254892831241\n",
      "\n",
      "Iteration 3200 lower bound -0.38855843491330644; gradient mag: 3.2484796161403118\n",
      "MEAN: -0.5404925168465505, VARIANCE: -0.35782866733184016\n",
      "\n",
      "Iteration 3300 lower bound -0.9605627660151979; gradient mag: 1.9285514959273473\n",
      "MEAN: -0.5454224765180294, VARIANCE: -0.363461306401235\n",
      "\n",
      "Iteration 3400 lower bound -0.6080186842232331; gradient mag: 1.8777535530359237\n",
      "MEAN: -0.550822695471604, VARIANCE: -0.3694364688328616\n",
      "\n",
      "Iteration 3500 lower bound -0.3404893659005286; gradient mag: 1.4533300791659218\n",
      "MEAN: -0.5558975704004226, VARIANCE: -0.3751867372259049\n",
      "\n",
      "Iteration 3600 lower bound -0.3719519277940657; gradient mag: 0.7901405029918478\n",
      "MEAN: -0.5609969335685575, VARIANCE: -0.38083718358024277\n",
      "\n",
      "Iteration 3700 lower bound -0.7286366378182325; gradient mag: 1.0163539847434646\n",
      "MEAN: -0.5661487648536911, VARIANCE: -0.3865077323125239\n",
      "\n",
      "Iteration 3800 lower bound -0.7340651639024431; gradient mag: 1.0078306664307886\n",
      "MEAN: -0.5706915627416055, VARIANCE: -0.3918199838849623\n",
      "\n",
      "Iteration 3900 lower bound -0.6236521203903151; gradient mag: 1.7179462167703277\n",
      "MEAN: -0.5752426305171982, VARIANCE: -0.3973290557098162\n",
      "\n",
      "Iteration 4000 lower bound -0.3891237876929228; gradient mag: 1.1642984267855712\n",
      "MEAN: -0.5797055275356721, VARIANCE: -0.4025479587794064\n",
      "\n",
      "Iteration 4100 lower bound -0.845803780833124; gradient mag: 0.4526537668978598\n",
      "MEAN: -0.5838864499099975, VARIANCE: -0.40766213634132686\n",
      "\n",
      "Iteration 4200 lower bound -0.6250694853142407; gradient mag: 1.9558723772736126\n",
      "MEAN: -0.5878771735139857, VARIANCE: -0.4126577970526699\n",
      "\n",
      "Iteration 4300 lower bound -0.676231955693851; gradient mag: 1.7760557391770164\n",
      "MEAN: -0.591435095100676, VARIANCE: -0.41723286232346324\n",
      "\n",
      "Iteration 4400 lower bound -0.39707485813901644; gradient mag: 0.47501965006392166\n",
      "MEAN: -0.5957666695101163, VARIANCE: -0.42250705561911034\n",
      "\n",
      "Iteration 4500 lower bound -0.4417742165231624; gradient mag: 1.3311373129832407\n",
      "MEAN: -0.5992798390118392, VARIANCE: -0.42700693003271173\n",
      "\n",
      "Iteration 4600 lower bound -0.44427064641784186; gradient mag: 0.1644468573750945\n",
      "MEAN: -0.6033817291811783, VARIANCE: -0.43230221146243436\n",
      "\n",
      "Iteration 4700 lower bound -0.5392801607180464; gradient mag: 0.4282172296819004\n",
      "MEAN: -0.6061234043983325, VARIANCE: -0.43647862007302923\n",
      "\n",
      "Iteration 4800 lower bound -0.49421625397157143; gradient mag: 1.2694196588191975\n",
      "MEAN: -0.6094671166383655, VARIANCE: -0.44128612291605507\n",
      "\n",
      "Iteration 4900 lower bound -0.7590218403135709; gradient mag: 0.3643187127773024\n",
      "MEAN: -0.6117850408533884, VARIANCE: -0.4449682056522804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = -20\n",
    "y = 1\n",
    "w = 0\n",
    "\n",
    "def log_density(w, t):\n",
    "    def sigmoid(w):\n",
    "        return 1/(1+np.exp(-(w*x + 10)))\n",
    "    return np.log(sigmoid(w)) + np.log(sp.stats.norm.pdf(w, 0, 1))\n",
    "\n",
    "verbose=True\n",
    "def callback(params, t, g):\n",
    "    if verbose:\n",
    "        if  t % 100 == 0:\n",
    "            print(\"Iteration {} lower bound {}; gradient mag: {}\".format(t, -objective(params, t), np.linalg.norm(gradient(params, t))))\n",
    "            print(\"MEAN: {}, VARIANCE: {}\\n\".format(params[0], params[1]))\n",
    "            \n",
    "D = 1\n",
    "init_mean = -0.3*np.ones(D)\n",
    "init_log_std = -0.11*np.ones(D)\n",
    "init_var_params = np.concatenate([init_mean, init_log_std])\n",
    "\n",
    "objective, gradient, unpack_params = black_box_variational_inference(log_density, D, 100)\n",
    "variational_params_1d = adam(gradient, init_var_params, step_size=1e-4, num_iters=5000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean field mean: -0.6146646213118641, Mean field standard deviation: 0.6380973134681004\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Mean field mean: {}, Mean field standard deviation: {}\".format(variational_params_1d[0], np.exp(variational_params_1d[1])))\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(np.random.normal(variational_params_1d[0], np.exp(variational_params_1d[1]), 1000), bins=int(np.sqrt(1000)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Question 1 was completed using the implementation of the log ensity from lecture 15. Similaryly, BBVI was implemented with the example from lecture 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define rbf activation function\n",
    "alpha = 1\n",
    "c = 0\n",
    "h = lambda x: np.exp(-alpha * (x - c)**2)\n",
    "\n",
    "###neural network model design choices\n",
    "width = 5\n",
    "hidden_layers = 1\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "architecture = {'width': width,\n",
    "               'hidden_layers': hidden_layers,\n",
    "               'input_dim': input_dim,\n",
    "               'output_dim': output_dim,\n",
    "               'activation_fn_type': 'rbf',\n",
    "               'activation_fn_params': 'c=0, alpha=1',\n",
    "               'activation_fn': h}\n",
    "\n",
    "#set random state to make the experiments replicable\n",
    "rand_state = 0\n",
    "random = np.random.RandomState(rand_state)\n",
    "\n",
    "#instantiate a Feedforward neural network object\n",
    "nn = Feedforward(architecture, random=random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Bayesian model parameterss\n",
    "#covariance matrix of the Gaussian prior on weights\n",
    "Sigma_W = 5**2 * np.eye(nn.D)\n",
    "#Gaussian likelihood variance\n",
    "sigma_y = 0.5**2\n",
    "#number of data points\n",
    "N = 12\n",
    "#precision matrix of the Gaussian prior on weights\n",
    "Sigma_W_inv = np.linalg.inv(Sigma_W)\n",
    "#determinant of the covaraince matrix of the Gaussian prior on weights\n",
    "Sigma_W_det = np.linalg.det(Sigma_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-afc793e3d99e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now need to do VI things\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./HW7_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "# Now need to do VI things\n",
    "data = pd.read_csv(\"./HW7_data.csv\")\n",
    "y_train = data['y'].values\n",
    "x_train = data['x'].values[np.newaxis,]\n",
    "\n",
    "# No uncertainty\n",
    "step_size = 1e-1\n",
    "S = 50\n",
    "\n",
    "print(\"STEP SIZE: {}, S: {}\".format(step_size, S))\n",
    "variational_params = variational_inference(Sigma_W, sigma_y, y_train, x_train, nn.forward, S=S, max_iteration=4001, step_size=step_size, verbose=True)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The plot of the neural network outputs are seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-8, 8, 100)\n",
    "fig, ax = plt.subplots()\n",
    "outputs = []\n",
    "sampled_weights = []\n",
    "for i in range(100):\n",
    "    weights = np.random.multivariate_normal(variational_params[:16], np.diag(np.exp(variational_params[16:])**2))\n",
    "    output = nn.forward(weights[np.newaxis,], xs[np.newaxis,])[0][0] + np.random.normal(0, 0.5**2, 100)\n",
    "    outputs.append(output)\n",
    "    sampled_weights.append(weights)\n",
    "\n",
    "lb_bayes = np.percentile(outputs, 2.5, axis=0)\n",
    "ub_bayes = np.percentile(outputs, 97.5, axis=0)\n",
    "ax.fill_between(xs, ub_bayes, lb_bayes, alpha=0.3)\n",
    "ax.scatter(x_train, y_train, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = nn.forward\n",
    "def log_lklhd(W):\n",
    "    S = W.shape[0]\n",
    "    constant = (-np.log(sigma_y) - 0.5 * np.log(2 * np.pi)) * N\n",
    "    exponential = -0.5 * sigma_y**-2 * np.sum((y_train.reshape((1, 1, N)) - forward(W, x_train))**2, axis=2).flatten()\n",
    "    return constant + exponential\n",
    "\n",
    "log_likelihoods = []\n",
    "for out in sampled_weights:\n",
    "    log_likelihoods.append(log_lklhd(out[:16][np.newaxis,]))\n",
    "print(\"4) POSTERIOR PREDICTIVE LOG-LIKELIHOOD: {}\".format(np.mean(log_likelihoods)))\n",
    "print(\"4) POSTERIOR PREDICTIVE LOG-LIKELIHOOD HW7 MODEL: -5.121776735095353\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We cannot really say if our posterior estimate is any good because our model missed a significant amount of epistemic uncertainty. Comparing it to the model from HW7, we see that model captures the epistemic and aleatoric uncertainty. Because of this, I would say that model is better. We would expect that the middle 95% predictive interval would be much larger where there is no data than where there is data. We do not see this. The model therefore captures the aleatoric uncertainty, but almost none of the epistemic uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Potential convergence of HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define rbf activation function\n",
    "alpha = 1\n",
    "c = 0\n",
    "h = lambda x: np.exp(-alpha * (x - c)**2)\n",
    "\n",
    "###neural network model design choices\n",
    "width = 5\n",
    "hidden_layers = 1\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "architecture = {'width': width,\n",
    "               'hidden_layers': hidden_layers,\n",
    "               'input_dim': input_dim,\n",
    "               'output_dim': output_dim,\n",
    "               'activation_fn_type': 'rbf',\n",
    "               'activation_fn_params': 'c=0, alpha=1',\n",
    "               'activation_fn': h}\n",
    "\n",
    "#set random state to make the experiments replicable\n",
    "rand_state = 0\n",
    "random = np.random.RandomState(rand_state)\n",
    "\n",
    "#instantiate a Feedforward neural network object\n",
    "nn = Feedforward(architecture, random=random)\n",
    "\n",
    "###define design choices in gradient descent\n",
    "params = {'step_size':1e-3, \n",
    "          'max_iteration':2000, \n",
    "          'random_restarts':1}\n",
    "\n",
    "#fit my neural network to minimize MSE on the given data\n",
    "nn.fit(x_train.reshape((1, -1)), y_train.reshape((1, -1)), params)\n",
    "\n",
    "x_test = np.linspace(-8, 8, 100)\n",
    "y_test = nn.forward(nn.weights, x_test.reshape((1, -1)))\n",
    "plt.scatter(x_train, y_train, color='black', label='data')\n",
    "plt.plot(x_test, y_test.flatten(), color='red', label='learned neural network function')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "copy_of_weights = np.copy(nn.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = nn.forward(copy_of_weights, x_train[np.newaxis,])\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.linspace(-8, 8, 100)\n",
    "ax.plot(xs, nn.forward(copy_of_weights, xs[np.newaxis,])[0][0])\n",
    "ax.scatter(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnn = Feedforward(architecture, random=random)\n",
    "def log_joint(w, mu=mu, sig1=5., y=y, sig2=0.5, N=16, nn=nnn):\n",
    "    print(w.shape)\n",
    "    sig1_mat = sig1**2 * np.eye(N)\n",
    "    mu = nn.forward(w, x_train[np.newaxis,])[0][0]\n",
    "    prior = np.log(sp.stats.multivariate_normal.pdf(w, np.zeros(N), sig1_mat))\n",
    "    likelihood = 0\n",
    "    for i in range(len(y_train)):\n",
    "        likelihood += np.log(sp.stats.norm.pdf(y_train[i], mu[i], sig2**2))\n",
    "    \n",
    "    return -prior - likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "def K(p, M):\n",
    "    d = len(p)\n",
    "    M_mat = 1/M*np.eye(len(p))\n",
    "    return 1/2*np.dot(p.T, np.dot(M_mat, p)) + 1/2*np.log(np.linalg.det(M_mat)) + d/2*np.log(2*np.pi)\n",
    "\n",
    "def U(q):\n",
    "    return np.array(np.log(2*np.pi) + np.linalg.norm(q)**2)/2\n",
    "\n",
    "def U_prime(q, u_func=U):\n",
    "    return 2*(q/u_func(q))\n",
    "\n",
    "def H(q, p, M, u_func=U):\n",
    "    return u_func(q) + K(p, M)\n",
    "\n",
    "def hist_data(data, idx):\n",
    "    return data[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(steps, eps, l_steps, M, q, N, u_func=U, u_prime_func=U_prime, nn=None, x=None):\n",
    "    acc = 0\n",
    "    p = np.random.randn((N))*M\n",
    "    if(nn == None):\n",
    "        new_u_func = lambda x: u_func(x)\n",
    "        u_prime_func = grad(new_u_func)\n",
    "    else:\n",
    "        q = np.copy(q[np.newaxis,])\n",
    "        new_u_func = lambda x: u_func(x)\n",
    "        u_prime_func = grad(new_u_func)\n",
    "\n",
    "    qs = [np.copy(q)]\n",
    "\n",
    "    for i in tqdm(range(steps)):\n",
    "        # Random p\n",
    "        p = np.random.randn((N))*M         \n",
    "        \n",
    "        p_prop = np.copy(p)\n",
    "        q_prop = np.copy(q)\n",
    "\n",
    "        # Leapfrog integration\n",
    "        p_prop -= eps/2*u_prime_func(np.array(q))[0]\n",
    "        for i in range(l_steps):\n",
    "            q_prop += eps/M*p_prop\n",
    "            p_prop -= eps*u_prime_func(np.array(q_prop))[0]\n",
    "        p_prop -= eps/2*u_prime_func(np.array(q_prop))[0]\n",
    "\n",
    "    \n",
    "        # Reverse momentum\n",
    "        p_prop = -p_prop\n",
    "    \n",
    "        alpha = K(p, M) - K(p_prop, M) + new_u_func(q) - new_u_func(q_prop)\n",
    "        if(alpha > np.log(np.random.random())):\n",
    "            acc += 1\n",
    "            q = np.copy(q_prop)\n",
    "            qs.append(q)\n",
    "        else:\n",
    "            qs.append(q)\n",
    "    \n",
    "    return np.array(qs), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'step_size':0.001, \n",
    "       'leapfrog_steps':50, \n",
    "       'total_samples':20, \n",
    "       'burn_in':.1, \n",
    "       'thinning_factor':2,\n",
    "       'position_init': np.copy(copy_of_weights)}\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.linspace(-8, 8, 100)\n",
    "plt.plot(xs, nnn.forward(copy_of_weights, xs[np.newaxis,])[0][0])\n",
    "plt.show()\n",
    "\n",
    "sampled_qs, acc = hmc(params['total_samples'], params['step_size'], params['leapfrog_steps'],\n",
    "             1, params['position_init'][0], len(params['position_init'][0]), log_joint, nn=nnn, x=x_train[0])\n",
    "\n",
    "burn_in = int(len(sampled_qs)*params['burn_in'])\n",
    "qs = np.array(sampled_qs[burn_in::params['thinning_factor']])\n",
    "print(\"ACCEPTANCE RATE: {}\".format(acc/params['total_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_nn = Feedforward(architecture, random=random)\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.linspace(-8, 8, 100)\n",
    "outs = []\n",
    "for i in range(100):\n",
    "    idx = np.random.choice(qs.shape[0])\n",
    "    posterior_predictive = np.array(qs[idx][np.newaxis,])\n",
    "    ys = new_nn.forward(posterior_predictive[0], xs[np.newaxis,])[0,0] + np.random.randn(100)*0.5**2\n",
    "    outs.append(ys)\n",
    "\n",
    "low_end = np.percentile(outs, 2.5, axis=0)\n",
    "high_end = np.percentile(outs, 97.5, axis=0)\n",
    "ax.fill_between(xs, low_end, high_end, alpha=0.3)\n",
    "_ = plt.scatter(x_train, y_train, color='black', label='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(mu, sigma=0.5):\n",
    "    return np.sum([np.log(sp.stats.norm.pdf(y_train[n], mu[n], sigma**2)) for n in range(len(mu))])\n",
    "\n",
    "lvs = []\n",
    "for i in range(len(qs)):\n",
    "    posterior_predictive = np.array(qs[i][np.newaxis,])\n",
    "    mu_vals = nn.forward(posterior_predictive[0], x_train[np.newaxis,])[0][0]\n",
    "    lvs.append(log_likelihood(mu_vals))\n",
    "\n",
    "print(\"5) POSTERIOR PREDICTIVE LOG-LIKELIHOOD: {}\".format(np.mean(lvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(qs[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
